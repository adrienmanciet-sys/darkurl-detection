{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dcc994",
   "metadata": {},
   "source": [
    "# ========================================================\n",
    "# Dark URL Detection\n",
    "# Adrien Manciet - Thibault Sourdeval\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15326439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f9c1c",
   "metadata": {},
   "source": [
    "Ce dataset est un ensemble d'url qui sont labellisés. Si le label vaut 1, l'url est dangereux, si il vaut -1, il ne l'est pas. L'objectif sera de faire un algorithme de classification des url en apprenant sur le dataset disponible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f0d314",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Partie 1 : Phase d'exploration\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48e021",
   "metadata": {},
   "source": [
    "Nous notons que les fichiers de données sont sous la forme de matrices sparse. Cela signifie que seules les valeurs non nulles sont gardées en mémoire. \n",
    "Cela permet d'épargner des erreurs de mémoire. \n",
    "\n",
    "Le fichier features contient des numéros qui semblent correspondre à des subdivisions de l'url contenant en blocs. Exemple : la première ligne du fichier features affiche 4, ce qui pourrait correspondre aux quatres premiers caractères de l'url 'http'. \n",
    "\n",
    "Nous codons une fonction de prévisualisation pour mieux comprendre la structure des données en les transformant en un dataframe. Pour la suite, \n",
    "nous resterons dans le format de données initial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14ca1d",
   "metadata": {},
   "source": [
    "**Fonction de prévisualisation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4f1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95710c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preview_data(day, nb_lines, nb_cols, random = True):\n",
    "    # Si random est laissé tel quel, une valeur au hasard \n",
    "    # est prise pour la première ligne et la première colonne à afficher\n",
    "\n",
    "    path =f\"url_svmlight/url_svmlight/Day{day}.svm\"\n",
    "    X, y = load_svmlight_file(path)\n",
    "    print(X.shape)\n",
    "\n",
    "    if random == True:\n",
    "        start_line = np.random.randint(0, len(y)-nb_lines)\n",
    "        start_col = np.random.randint(0,X.shape[1])\n",
    "    else : \n",
    "        start_line = int(input(\"Première ligne à afficher : \"))\n",
    "        start_col = int(input(\"Première colonne à afficher : \"))\n",
    "    \n",
    "    label_list = []\n",
    "    for i in range(start_col, start_col+nb_cols):\n",
    "        label_list.append(i)\n",
    "\n",
    "    X_df = pd.DataFrame(X[start_line: start_line+nb_lines, start_col: start_col+nb_cols].toarray(), columns=label_list)\n",
    "    y_df = pd.DataFrame(y[start_line: start_line+nb_lines], columns=['label'])\n",
    "\n",
    "    data = pd.concat([X_df, y_df], axis=1)\n",
    "    return data\n",
    "\n",
    "preview_data(17, 10, 10, random=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844f418",
   "metadata": {},
   "source": [
    "**Visualisations grahiques** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(day, feature_x, feature_y): \n",
    "    path =f\"url_svmlight/url_svmlight/Day{day}.svm\"\n",
    "    X, y = load_svmlight_file(path)\n",
    "\n",
    "    X = X[0:1000,:1000].toarray()\n",
    "    plt.figure()\n",
    "    plt.grid(alpha=0.2)\n",
    "    sc = plt.scatter(X[:,feature_x], X[:,feature_y], c=y[0:1000], cmap=\"viridis\")\n",
    "    plt.xlabel(f'Feature {feature_x}')\n",
    "    plt.ylabel(f'Feature {feature_y}')\n",
    "    \n",
    "\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label(\"Label\")\n",
    "    plt.show()\n",
    "\n",
    "scatter_plot(11,4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea2fd2",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Partie 2 : Feature Engineering\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971170d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Réduction de features\n",
    "# =============================\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"\\n--- Réduction des features ---\")\n",
    "\n",
    "path =f\"url_svmlight/url_svmlight/Day96.svm\"\n",
    "X, y = load_svmlight_file(path)\n",
    "\n",
    "# (a) Supprimer les colonnes complètement nulles\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_reduced = vt.fit_transform(X)\n",
    "\n",
    "print(\"Shape après suppression des colonnes nulles :\", X_reduced.shape)\n",
    "\n",
    "# (b) Supprimer les features très rares (optionnel mais utile)\n",
    "print(\"Suppression des features très rares (présentes < 5 fois)...\")\n",
    "X_csc = X_reduced.tocsc()\n",
    "feature_counts = np.diff(X_csc.indptr)\n",
    "mask = feature_counts >= 5\n",
    "X_reduced = X_csc[:, mask].tocsr()\n",
    "print(\"Shape après suppression des features rares :\", X_reduced.shape)\n",
    "\n",
    "print(\"\\n--- PCA ---\")\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "plt.scatter(X_svd[:,0], X_svd[:,1], c=y, cmap='coolwarm', s=10, alpha=0.6)\n",
    "plt.xlabel('Composante 1')\n",
    "plt.ylabel('Composante 2')\n",
    "plt.title('Projection des URLs sur les 2 premières composantes SVD')\n",
    "plt.colorbar(label='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ebb12",
   "metadata": {},
   "source": [
    "Nous commençons par une pca brute pour voir ce que cela peut donner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0ed61",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c802d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "X, y = load_svmlight_file('url_svmlight/url_svmlight/Day96.svm')\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "# print(X_svd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d021cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd.explained_variance_ratio_)\n",
    "plt.scatter(X_svd[:1000,0], X_svd[:1000,1], c=y[:1000], cmap='coolwarm', s=10, alpha=0.6)\n",
    "plt.xlabel('Composante 1')\n",
    "plt.ylabel('Composante 2')\n",
    "plt.title('Projection des URLs sur les 2 premières composantes SVD')\n",
    "plt.colorbar(label='label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2607d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((2,3))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67541a",
   "metadata": {},
   "source": [
    "Constatant l'efficacité toute relative de cette PCA, nous décidons de retravailler sur les données d'entrées afin d'éliminer dès le départ des features à trop faible variance.\n",
    "Nous remarquons que beaucoup de colonnes sont nulles sur la prévisualisation, il faut les retirer du dataset. \n",
    "La difficulté est de parcourir tous les fichiers svm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a54207",
   "metadata": {},
   "source": [
    "Comme beaucoup de nos colonnes de features ne contiennent que des 0 ou des 1, et que cela ne permettra pas la classification, nous décidons d'enlever ces colonnes dans les données qui serveront à l'apprentissage. \n",
    "\n",
    "Pour ce faire, nous utilisons un critère sur la variance minimale d'une colonne dans chaque fichier. Puis, nous regardons le nombre de fichier pour lesquels une colonne a été gardée. Nous mesurons cela en pourcentage. Exemple : la colonne 1 a une variance supérieure au critère minimal dans les fichiers 1 à 10, mais pas dans les fichiers 11 à 20. Ainsi, l'algorithme a gardé la colonne un pour les fichiers 1 à 10 et l'a enlevée dans les autres. Au total, la colonne 1 a été gardée dans 50% des cas.\n",
    "\n",
    "Dans un premier temps, nous gardons les colonnes dès lors qu'elles sont gardées au moins une fois, soit que leur pourcentage d'apparition est strictement positif. Nous pourrons raffiner cela pour garder moins de features si on voit que cela améliore la performance de la méthode d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee128f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import os\n",
    "\n",
    "data_dir = \"url_svmlight/url_svmlight\"\n",
    "X_list, y_list = [], []\n",
    "\n",
    "\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "for file in sorted(os.listdir(data_dir))[:10]:  # exemple sur 10 jours\n",
    "    X, y = load_svmlight_file(os.path.join(data_dir, file))\n",
    "    # print('passage')\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "\n",
    "\n",
    "kept_mask = []\n",
    "for i in range(len(X_list)):\n",
    "    X_reduced = selector.fit_transform(X_list[i])\n",
    "    print(X_list[i].shape, \"→\", X_reduced.shape)\n",
    "    keep_mask = selector.get_support()\n",
    "    kept_mask.append(np.where(keep_mask)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b06e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import os\n",
    "\n",
    "data_dir = \"url_svmlight/url_svmlight\"\n",
    "X_list, y_list = [], []\n",
    "\n",
    "max_features = 3300000  # à adapter à ton dataset\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "kept_mask_list = []\n",
    "\n",
    "# Boucle sur les fichiers\n",
    "for file in sorted(os.listdir(data_dir))[:10]:  # exemple sur 10 jours\n",
    "    X, y = load_svmlight_file(os.path.join(data_dir, file), n_features=max_features)\n",
    "    X_list.append(X)\n",
    "    y_list.append(y)\n",
    "\n",
    "    X_reduced = selector.fit_transform(X)\n",
    "    keep_mask = selector.get_support()  # booléen : True si la colonne est gardée\n",
    "    kept_mask_list.append(keep_mask)\n",
    "\n",
    "# Transformer en matrice 2D : fichiers × colonnes\n",
    "kept_mask_matrix = np.array(kept_mask_list, dtype=int)\n",
    "\n",
    "# Pourcentage de fois où chaque colonne est gardée\n",
    "column_keep_percentage = kept_mask_matrix.mean(axis=0) * 100\n",
    "\n",
    "# Créer DataFrame en ne gardant que les colonnes qui ont été sélectionnées au moins une fois\n",
    "df_keep = pd.DataFrame({\n",
    "    'column_index': np.arange(len(column_keep_percentage)),\n",
    "    'percent_kept': column_keep_percentage\n",
    "})\n",
    "\n",
    "# Filtrer les colonnes jamais gardées\n",
    "df_keep = df_keep[df_keep['percent_kept'] > 0]\n",
    "# Trier par pourcentage décroissant si tu veux\n",
    "# df_keep = df_keep.sort_values(by='percent_kept', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Afficher un aperçu\n",
    "# print(df_keep.head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8bdb4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_dir)])[:\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# ou plus\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Lecture en une seule fois : bien plus rapide que 10 load_svmlight_file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m X_all, y_all \u001b[38;5;241m=\u001b[39m load_svmlight_files(files, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3_300_000\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Si load_svmlight_files renvoie plusieurs sorties, les empiler :\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_all, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "data_dir = \"url_svmlight/url_svmlight\"\n",
    "\n",
    "# Liste complète des chemins\n",
    "files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir)])[:2]  # ou plus\n",
    "\n",
    "# Lecture en une seule fois : bien plus rapide que 10 load_svmlight_file\n",
    "X_all, y_all = load_svmlight_files(files, n_features=3_300_000)\n",
    "\n",
    "# Si load_svmlight_files renvoie plusieurs sorties, les empiler :\n",
    "if isinstance(X_all, tuple):\n",
    "    X_all = vstack(X_all)\n",
    "    y_all = np.hstack(y_all)\n",
    "\n",
    "# Sélection de variance\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_reduced = selector.fit_transform(X_all)\n",
    "\n",
    "keep_mask = selector.get_support()\n",
    "kept_indices = np.where(keep_mask)[0]\n",
    "\n",
    "print(f\"{keep_mask.sum()} variables conservées sur {keep_mask.size}\")\n",
    "\n",
    "# Si tu veux conserver le pourcentage ou l’importance\n",
    "variances = selector.variances_\n",
    "df_keep = pd.DataFrame({\n",
    "    'column_index': np.arange(len(variances)),\n",
    "    'variance': variances,\n",
    "    'kept': keep_mask\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1642bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = \"url_svmlight/url_svmlight\"\n",
    "files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir)])[:20]\n",
    "\n",
    "max_features = 3_300_000\n",
    "mean_ = np.zeros(max_features)\n",
    "mean_sq_ = np.zeros(max_features)\n",
    "n_total = 0\n",
    "\n",
    "for file in files:\n",
    "    X, _ = load_svmlight_file(file, n_features=max_features)\n",
    "    n_samples = X.shape[0]\n",
    "    mean_ += X.mean(axis=0).A1 * n_samples\n",
    "    mean_sq_ += (X.power(2).mean(axis=0).A1) * n_samples\n",
    "    n_total += n_samples\n",
    "\n",
    "mean_ /= n_total\n",
    "mean_sq_ /= n_total\n",
    "variances = mean_sq_ - mean_**2\n",
    "keep_mask = variances > 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e194ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "print(keep_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9de58",
   "metadata": {},
   "source": [
    "Sélection des données d'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268b4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "selected_columns = df_keep['column_index'].values\n",
    "\n",
    "X_filtered_list, y_filtered_list = [],[]\n",
    "\n",
    "for file in sorted(os.listdir(data_dir))[:10]:\n",
    "    X,y = load_svmlight_file(os.path.join(data_dir, file), n_features=max_features)\n",
    "\n",
    "    X_filtered = X[:,selected_columns]\n",
    "    X_filtered_list.append(X_filtered)\n",
    "    y_filtered_list.append(y)\n",
    "\n",
    "\n",
    "X_all = sp.vstack(X_filtered_list)\n",
    "y_all = np.concatenate(y_filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6422b",
   "metadata": {},
   "source": [
    "**PCA sur les données filtrées**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# X_reduced_scaled = StandardScaler().fit_transform(X_reduced)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X_reduced_scaled)\n",
    "# X_reduced_proj = pca.transform(X_reduced_scaled)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.scatter(X_reduced[:,15], X_reduced[:,16],c = y, cmap='viridis', alpha=0.5)\n",
    "# plt.grid(alpha=0.2)\n",
    "# plt.title('Dans les coordonées de base')\n",
    "# plt.colorbar(label='label')\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.scatter(X_reduced_proj[:,0], X_reduced_proj[:,1], c = y, cmap='viridis', alpha=0.5)\n",
    "# plt.xlabel('Première composante principale')\n",
    "# plt.ylabel('Deuxième composante principale')\n",
    "# plt.colorbar(label='label')\n",
    "# plt.grid(alpha=0.2)\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac87ff4",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Partie 3 : Phase d'apprentissage\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "\n",
    "# ⚠️ Important : un kernel RBF ne gère pas directement les matrices sparse.\n",
    "# On convertit donc un échantillon en dense (en RAM attention)\n",
    "# Si ton dataset est trop gros, on prend un sous-échantillon.\n",
    "if X_reduced.shape[0] > 5000 or X_reduced.shape[1] > 5000:\n",
    "    print(\"Dataset trop volumineux — on prend un échantillon de 5000 URLs pour la démonstration.\")\n",
    "    from sklearn.utils import resample\n",
    "    X_reduced, y = resample(X_reduced, y, n_samples=5000, random_state=42)\n",
    "    X_reduced = X_reduced.toarray()  # conversion en dense\n",
    "else:\n",
    "    X_reduced = X_reduced.toarray()\n",
    "\n",
    "# =========================\n",
    "# 2️⃣ Séparer train/test\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Normalisation recommandée pour SVM RBF\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# =========================\n",
    "# 3️⃣ Entraînement SVM (kernel trick)\n",
    "# =========================\n",
    "print(\"\\n--- Entraînement du SVM avec noyau RBF ---\")\n",
    "\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',     # 'linear', 'poly', 'rbf', 'sigmoid'...\n",
    "    C=1.0,            # paramètre de régularisation\n",
    "    gamma='scale',    # influence du noyau RBF (auto ou 'scale')\n",
    "    class_weight='balanced',  # utile si les classes sont déséquilibrées\n",
    ")\n",
    "\n",
    "t0 = time()\n",
    "svm_model.fit(X_train, y_train)\n",
    "t1 = time()\n",
    "\n",
    "print(f\"✅ Modèle entraîné en {t1 - t0:.2f} secondes\")\n",
    "\n",
    "# =========================\n",
    "# 4️⃣ Évaluation\n",
    "# =========================\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Évaluation du modèle ---\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nMatrice de confusion :\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nRapport de classification :\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760e954",
   "metadata": {},
   "source": [
    "**Revue des méthodes qui pourraient être utiles**\n",
    "\n",
    "Nous pouvons déjà rejeter le bagging, car notre jeu de données contient largement assez d'échantillons. Il reste svm, l'inférence bayésienne, la régression logistique (si multi-linéarité), les arbres de décisions (XGboost), les processsus gaussiens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1278a",
   "metadata": {},
   "source": [
    "Commençons par un svm naïf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0316ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f83e0a",
   "metadata": {},
   "source": [
    "Entraînement pur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "print('Précision de : ', accuracy_score(y_test, y_pred))\n",
    "print(\"\\nRésumé de classification:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nMatrice de confusion:\\n\", confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7f070",
   "metadata": {},
   "source": [
    "Tentative de validation croisée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    MaxAbsScaler(),           # scaler rapide pour sparse matrices\n",
    "    LinearSVC(\n",
    "        dual=False,           # plus rapide si n_samples > n_features\n",
    "        max_iter=3000,        # limite d'itérations\n",
    "        tol=1e-3,             # tolérance plus élevée pour converger plus vite\n",
    "        random_state=42\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X_all, y_all, cv=4, n_jobs=-1)\n",
    "\n",
    "print(\"Scores sur les folds :\", scores)\n",
    "print(\"Précision moyenne :\", np.mean(scores))\n",
    "print(\"Écart-type :\", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ad290",
   "metadata": {},
   "source": [
    "**SVM avec un kernel non linéaire**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "# Pipeline pour gérer le sparse matrix\n",
    "model = make_pipeline(\n",
    "    MaxAbsScaler(),                # scaler rapide compatible sparse\n",
    "    SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)  # kernel RBF\n",
    ")\n",
    "\n",
    "# Cross-validation sur un échantillon réduit pour tester\n",
    "sample_idx = np.random.choice(X_all.shape[0], size=100_000, replace=False)\n",
    "X_sample = X_all[sample_idx]\n",
    "y_sample = y_all[sample_idx]\n",
    "\n",
    "scores = cross_val_score(model, X_sample, y_sample, cv=3, n_jobs=-1)\n",
    "print(\"Scores CV :\", scores)\n",
    "print(\"Précision moyenne :\", np.mean(scores))\n",
    "print(\"Écart-type :\", np.std(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcd8f4",
   "metadata": {},
   "source": [
    "Nous constatons une légère amélioration de la précision obtenue avec un kernel rbf. Néanmoins, le temps de calcul est nettement plus long. Vu la taille du jeu de données qu'on souhaite tester, il n'est pas envisageable de prendre le rbf tel quel. Il pourrait convenir si on avait moins de features. Mais, dans ce cas, n'aurait-on pas un soucis de nombre de samples par rapport au nombre de features ? \n",
    "\n",
    "Etant donné qu'on obtient des résultats satisfaisants avec le svm linéaire, une bonne précision, sans surapprentissage (nous l'avons vérifié avec la validation croisée), il paraît assez optimal de continuer avec le svm linéaire. Quel paramètre peut-on ajuster pour la suite ? \n",
    "\n",
    "Peut-être faut-il interroger le nombre de features sélectionné avec le critère sur la variance ? Nous souhaitions aussi implémenter une autre technique de sélection de variable consistant à évaluer la pertinence d'une feature sur la prédiction de classe du sample (avec un test du chi2 par exemple). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f6aa9",
   "metadata": {},
   "source": [
    "**Test de la régression logisitique :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281ac7e",
   "metadata": {},
   "source": [
    "Etant donné que le svm linéaire donne des résultats satisfaisants, essayons une méthode de classification linéaire qui a l'avantage de pouvoir gérer de grands volumes de données, ce qui est notre cas ici. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778eb312",
   "metadata": {},
   "source": [
    "*Régression L1 pour sélectionner les features les plus prédictives*\n",
    "\n",
    "L'idée est de garder le moins de features possible par régression L1 pour accélerer l'execution de la régression L2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bea4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf_l1 = LogisticRegression(\n",
    "#     penalty='l1',\n",
    "#     solver='saga',      # gère sparse et grands datasets\n",
    "#     max_iter=500,        # ajustable pour la vitesse\n",
    "#     C=0.1,               # régularisation forte → plus de coefficients à zéro\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# clf_l1.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef = clf_l1.coef_[0]\n",
    "# important_idx = np.where(coef != 0)[0]\n",
    "\n",
    "# Si tu veux les noms des features (X_train doit être un DataFrame)\n",
    "# important_features = np.array(X_train.columns)[important_idx]\n",
    "# print(\"Nombre de features importantes :\", len(important_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_reduced = X_train_scaled[:, important_idx]\n",
    "# X_test_reduced = X_test_scaled[:, important_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_l2 = LogisticRegression(\n",
    "#     penalty='l2',\n",
    "#     solver='saga',\n",
    "#     max_iter=1000,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# clf_l2.fit(X_train_reduced, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# y_pred = clf_l2.predict(X_test_reduced)\n",
    "# y_prob = clf_l2.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "# print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "# print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97f901",
   "metadata": {},
   "source": [
    "Après mise en place de la méthode L1 puis L2, on se rend compte que la méthode L1 est plus lente que la L2 et donc qu'on perd plus de temps à sélectionner les variables prédictives en mettant les autres à 0 au moyen d'un régression L1 qu'à faire directement une régression L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d08191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',       # ou 'l1' pour sélection de features\n",
    "    solver='saga',      # efficace pour grands datasets\n",
    "    max_iter=1000,      # augmenter si convergence lente\n",
    "    n_jobs=-1           # parallélisation\n",
    ")\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, clf.predict_proba(X_test_scaled)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5955bd",
   "metadata": {},
   "source": [
    "**Inférence Bayésienne**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b2a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9035969387755102\n",
      "AUC: 0.9609527940606886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# alpha = 1 correspond à un prior Beta(1,1)\n",
    "model = BernoulliNB(alpha=1.0, binarize=None)  # binarize=None car tes features sont déjà binaires\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, y_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e00140",
   "metadata": {},
   "source": [
    "Approche par batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9ea842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8982\n",
      "AUC : 0.9589\n",
      "Matrice de confusion :\n",
      " [[12383   513]\n",
      " [ 1522  5582]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "data_dir = \"url_svmlight/url_svmlight\"\n",
    "max_features = 3_300_000\n",
    "keep_mask = np.array(keep_mask)\n",
    "selected_columns = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "# Modèle\n",
    "model = BernoulliNB(alpha=1.0)\n",
    "classes = np.array([-1,1])\n",
    "\n",
    "# Split train/test\n",
    "# Ici, on réserve le dernier fichier pour test\n",
    "all_files = sorted([os.path.join(data_dir,f) for f in os.listdir(data_dir)])[:20]\n",
    "train_files = all_files[:-1]\n",
    "test_file = all_files[-1]\n",
    "\n",
    "# --- Entraînement batch par batch ---\n",
    "for file in train_files:\n",
    "    X, y = load_svmlight_file(file, n_features=max_features)\n",
    "    X_filtered = X[:, selected_columns]\n",
    "    model.partial_fit(X_filtered, y, classes=classes)\n",
    "\n",
    "# --- Evaluation sur le fichier test ---\n",
    "X_test, y_test = load_svmlight_file(test_file, n_features=max_features)\n",
    "X_test_filtered = X_test[:, selected_columns]\n",
    "\n",
    "y_pred = model.predict(X_test_filtered)\n",
    "y_proba = model.predict_proba(X_test_filtered)[:,1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"AUC : {auc:.4f}\")\n",
    "print(\"Matrice de confusion :\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662aa27",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Partie 4 : Tuning d'un hyperparamètre\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb813cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cd04f",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Partie 5 : Conclusions\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130342e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =f\"url_svmlight/url_svmlight/Day1.svm\"\n",
    "X, y = load_svmlight_file(path)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "\n",
    "# =============================\n",
    "# 1️⃣ Charger le fichier .svm\n",
    "# =============================\n",
    "print(\"Chargement du dataset...\")\n",
    "X, y = load_svmlight_file(path)  # <-- à adapter à ton chemin\n",
    "print(f\"Shape initial : {X.shape}\")\n",
    "print(f\"Nombre d'éléments non nuls : {X.nnz}\")\n",
    "print(f\"Taux de sparsité : {100*(1 - X.nnz/(X.shape[0]*X.shape[1])):.6f}%\")\n",
    "\n",
    "# Mélanger pour éviter un ordre biaisé\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# =============================\n",
    "# 2️⃣ Séparer train/test\n",
    "# =============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =============================\n",
    "# 3️⃣ Entraînement avant nettoyage\n",
    "# =============================\n",
    "print(\"\\n--- Entraînement AVANT réduction ---\")\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Temps d'entraînement : {t1 - t0:.2f}s\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# =============================\n",
    "# 4️⃣ Réduction de features\n",
    "# =============================\n",
    "print(\"\\n--- Réduction des features ---\")\n",
    "\n",
    "# (a) Supprimer les colonnes complètement nulles\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_reduced = vt.fit_transform(X)\n",
    "\n",
    "print(\"Shape après suppression des colonnes nulles :\", X_reduced.shape)\n",
    "\n",
    "# (b) Supprimer les features très rares (optionnel mais utile)\n",
    "print(\"Suppression des features très rares (présentes < 5 fois)...\")\n",
    "X_csc = X_reduced.tocsc()\n",
    "feature_counts = np.diff(X_csc.indptr)\n",
    "mask = feature_counts >= 5\n",
    "X_reduced = X_csc[:, mask].tocsr()\n",
    "print(\"Shape après suppression des features rares :\", X_reduced.shape)\n",
    "\n",
    "# =============================\n",
    "# 5️⃣ Réentraînement après réduction\n",
    "# =============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n--- Entraînement APRÈS réduction ---\")\n",
    "clf2 = LogisticRegression(max_iter=1000)\n",
    "t0 = time()\n",
    "clf2.fit(X_train, y_train)\n",
    "t1 = time()\n",
    "y_pred = clf2.predict(X_test)\n",
    "print(f\"Temps d'entraînement : {t1 - t0:.2f}s\")\n",
    "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\n✅ Script terminé avec succès.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
